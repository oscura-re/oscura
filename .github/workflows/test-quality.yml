name: Test Quality Gates

on:
  pull_request:
    branches: [main]
  push:
    branches: [main]
  workflow_dispatch:
  schedule:
    # Run weekly on Sunday at 4 AM UTC to detect test order dependencies
    - cron: "0 4 * * 0"

# Auto-cancel outdated runs for the same branch/PR
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

# Minimal permissions for security
permissions:
  contents: read
  pull-requests: read
  checks: write

env:
  # Artifact retention policy (days)
  RETENTION_MEDIUM: 30 # Test quality reports

  PYTHON_VERSION: "3.12"

jobs:
  # ==========================================================================
  # Path Detection - Check if test quality checks should run
  # ==========================================================================
  detect-changes:
    name: Detect Test Changes
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      should-run: ${{ steps.filter.outputs.tests }}
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            tests:
              - 'tests/**'
              - 'pyproject.toml'
              - 'scripts/testing/**'
              - '.github/workflows/test-quality.yml'

  # ==========================================================================
  # Marker Validation
  # ==========================================================================
  marker-validation:
    name: Validate Test Markers
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [detect-changes]
    if: needs.detect-changes.outputs.should-run == 'true'
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Setup Python Environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          extras: "--dev"

      - name: Validate markers
        run: |
          uv run python scripts/testing/validate_test_markers.py

      - name: Check for unregistered markers
        if: failure()
        run: |
          echo "::error::Test marker validation failed. Run 'uv run python \
          scripts/testing/validate_test_markers.py --fix' to auto-fix missing markers."

  # ==========================================================================
  # Test Isolation Check
  # ==========================================================================
  test-isolation:
    name: Check Test Isolation
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [detect-changes]
    if: needs.detect-changes.outputs.should-run == 'true'
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Setup Python Environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Check isolation (sample 15 files)
        run: |
          uv run python scripts/testing/check_test_isolation.py --sample 15
        continue-on-error: false # Strict isolation enforcement

      - name: Upload isolation check results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: isolation-check-results
          path: |
            isolation-*.log
          retention-days: ${{ env.RETENTION_MEDIUM }}
          if-no-files-found: ignore

  # ==========================================================================
  # Test Coverage Markers
  # ==========================================================================
  coverage-markers:
    name: Verify Coverage Markers
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [detect-changes]
    if: needs.detect-changes.outputs.should-run == 'true'
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Setup Python Environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          install-deps: "false"

      - name: Check marker distribution
        run: |
          uv run python scripts/testing/validate_test_markers.py | tee marker-report.txt

      - name: Analyze marker coverage
        run: |
          echo "=== Marker Distribution Summary ==="
          grep -A 10 "Level Marker Distribution:" marker-report.txt || true
          echo ""
          echo "=== Files Missing Markers ==="
          grep "Files missing required markers:" marker-report.txt || true

      - name: Upload marker report
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: marker-distribution-report
          path: marker-report.txt
          retention-days: ${{ env.RETENTION_MEDIUM }}

  # ==========================================================================
  # Randomized Test Order (weekly)
  # ==========================================================================
  randomized-tests:
    name: Randomized Test Order Detection
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Setup Python Environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Run tests with random order (run 1)
        run: |
          uv run python -m pytest tests/unit \
            -v \
            -m "unit and not slow and not memory_intensive" \
            -n 4 \
            --maxprocesses=4 \
            --dist loadscope \
            --hypothesis-profile=ci \
            --benchmark-disable \
            --tb=short \
            --maxfail=20 \
            --junit-xml=randomized-run1.xml

      - name: Run tests with random order (run 2)
        run: |
          uv run python -m pytest tests/unit \
            -v \
            -m "unit and not slow and not memory_intensive" \
            -n 4 \
            --maxprocesses=4 \
            --dist loadscope \
            --hypothesis-profile=ci \
            --benchmark-disable \
            --tb=short \
            --maxfail=20 \
            --junit-xml=randomized-run2.xml

      - name: Run tests with random order (run 3)
        run: |
          uv run python -m pytest tests/unit \
            -v \
            -m "unit and not slow and not memory_intensive" \
            -n 4 \
            --maxprocesses=4 \
            --dist loadscope \
            --hypothesis-profile=ci \
            --benchmark-disable \
            --tb=short \
            --maxfail=20 \
            --junit-xml=randomized-run3.xml

      - name: Upload randomized test results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: randomized-test-results
          path: |
            randomized-*.xml
          retention-days: ${{ env.RETENTION_MEDIUM }}

      - name: Create issue if failures detected
        if: failure()
        uses: actions/github-script@v8
        with:
          script: |
            const body = `## Test Order Dependency Detected

            The weekly randomized test run has detected potential test order dependencies.

            **What this means:**
            - Some tests may be passing/failing based on execution order
            - Tests should be isolated and not depend on state from other tests

            **Next steps:**
            1. Review the failed test results in the workflow artifacts
            2. Identify which tests failed with randomized order
            3. Fix the test isolation issues
            4. Use \`pytest --randomly-seed=last\` to reproduce the failure locally

            **Workflow run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
            `;

            // Check if issue already exists
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'test-order-dependency'
            });

            if (issues.data.length === 0) {
              // Create new issue
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: '[Test Quality] Test Order Dependencies Detected',
                body: body,
                labels: ['test-order-dependency', 'bug', 'testing']
              });
            }

  # ==========================================================================
  # Final status check - ALWAYS RUNS
  # ==========================================================================
  quality-gates-success:
    name: Test Quality Gates
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [detect-changes, marker-validation, test-isolation, coverage-markers]
    if: always()
    steps:
      - name: Check test quality results
        run: |
          echo "Detect Changes: ${{ needs.detect-changes.result }}"
          echo "Should Run: ${{ needs.detect-changes.outputs.should-run }}"
          echo "Marker Validation: ${{ needs.marker-validation.result }}"
          echo "Test Isolation: ${{ needs.test-isolation.result }}"
          echo "Coverage Markers: ${{ needs.coverage-markers.result }}"

          # If no test changes, all quality checks will be skipped (not failed)
          if [[ "${{ needs.detect-changes.outputs.should-run }}" != "true" ]]; then
            echo "No test changes detected - quality checks skipped"
            echo "✅ Test Quality Gates passed (no test changes to check)"
            exit 0
          fi

          # Marker validation must pass (if it ran)
          if [[ "${{ needs.marker-validation.result }}" == "failure" ]]; then
            echo "::error::Marker validation failed - this is a required gate"
            exit 1
          fi

          # Other checks can warn but not fail
          if [[ "${{ needs.test-isolation.result }}" == "failure" ]]; then
            echo "::warning::Test isolation check found issues"
          fi

          if [[ "${{ needs.coverage-markers.result }}" == "failure" ]]; then
            echo "::warning::Coverage marker check had issues"
          fi

          echo "✅ Test Quality Gates passed!"
          exit 0
