name: CI

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

# Auto-cancel outdated runs for the same branch/PR
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

# Minimal permissions for security
permissions:
  contents: read
  pull-requests: read
  checks: write

env:
  PYTHON_VERSION: "3.12"
  UV_CACHE_DIR: /tmp/.uv-cache
  # Artifact retention policy (days)
  RETENTION_SHORT: 14 # Test results, temporary artifacts
  RETENTION_MEDIUM: 30 # Code quality, benchmarks
  RETENTION_LONG: 90 # Security, compliance reports

jobs:
  # ============================================================================
  # Stage 1: Fast checks (run in parallel, fail fast)
  # ============================================================================

  pre-commit:
    name: Pre-Commit Hooks
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Set up Node.js
        uses: actions/setup-node@v6
        with:
          node-version: "20"

      - name: Setup Python Environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pre-commit
        uses: actions/cache@v5
        with:
          path: ~/.cache/pre-commit
          key: pre-commit-${{ runner.os }}-${{ hashFiles('.pre-commit-config.yaml') }}
          restore-keys: |
            pre-commit-${{ runner.os }}-

      - name: Install pre-commit
        run: pip install pre-commit

      - name: Run pre-commit hooks
        run: |
          # Run pre-commit and capture exit code and output
          set +e
          PRE_COMMIT_OUTPUT=$(pre-commit run --all-files --show-diff-on-failure 2>&1)
          PRECOMMIT_EXIT=$?
          set -e

          echo "$PRE_COMMIT_OUTPUT"

          # If pre-commit made changes, check if only badges changed
          if [ $PRECOMMIT_EXIT -ne 0 ]; then
            if git diff --name-only | grep -q '^docs/badges/'; then
              echo "⚠️ Badge files were auto-updated by interrogate"
              # If ONLY badge files changed, this is OK
              if [ $(git diff --name-only | wc -l) -eq $(git diff --name-only | grep '^docs/badges/' | wc -l) ]; then
                echo "✅ Only badge files changed - allowing"
                exit 0
              fi
            fi

            # Otherwise fail
            exit $PRECOMMIT_EXIT
          fi

  lint:
    name: Lint
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Setup Python Environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Run ruff check
        run: uv run ruff check src/ tests/

      - name: Run ruff format check
        run: uv run ruff format --check src/ tests/

  typecheck:
    name: Type Check
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Setup Python Environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache mypy
        uses: actions/cache@v5
        with:
          path: .mypy_cache
          key: mypy-${{ runner.os }}-${{ env.PYTHON_VERSION }}-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            mypy-${{ runner.os }}-${{ env.PYTHON_VERSION }}-

      - name: Run mypy
        run: uv run mypy src/

  config-validation:
    name: Config Validation
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Setup Python Environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          extras: "--no-dev"

      - name: Validate orchestration config consistency
        run: uv run python .claude/hooks/validate_config_consistency.py

      - name: Validate SSOT
        run: uv run python .claude/hooks/validate_ssot.py

      - name: Run hook unit tests
        run: uv run python tests/unit/hooks/test_hooks_comprehensive.py

  # ============================================================================
  # Stage 2: Tests (depend on lint passing)
  # ============================================================================

  test:
    name: Test (Python ${{ matrix.python-version }}, ${{ matrix.test-group }})
    runs-on: ubuntu-latest
    timeout-minutes: 25
    needs: [lint] # Only needs lint - typecheck can run in parallel
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.12", "3.13"]
        test-group:
          - "analyzers"
          - "core-protocols-loaders"
          - "search-filtering-streaming"
          - "cli-ui-reporting"
          - "unit-workflows"
          - "unit-exploratory"
          - "unit-utils"
          - "non-unit-tests"
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Setup Python Environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pytest
        uses: actions/cache@v5
        with:
          path: .pytest_cache
          key: pytest-${{ runner.os }}-${{ matrix.python-version }}-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            pytest-${{ runner.os }}-${{ matrix.python-version }}-

      - name: Cache Hypothesis examples
        uses: actions/cache@v5
        with:
          path: .hypothesis
          key: hypothesis-${{ runner.os }}-${{ matrix.python-version }}-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            hypothesis-${{ runner.os }}-${{ matrix.python-version }}-
            hypothesis-${{ runner.os }}-

      - name: Verify test data (optional)
        run: ./scripts/testing/verify_test_data.sh
        continue-on-error: true

      - name: Determine test paths
        id: test-paths
        run: |
          case "${{ matrix.test-group }}" in
            "analyzers")
              echo "paths=tests/unit/analyzers/" >> $GITHUB_OUTPUT
              ;;
            "core-protocols-loaders")
              paths="tests/unit/core/ tests/unit/protocols/"
              paths="$paths tests/unit/loaders/ tests/unit/math/"
              paths="$paths tests/unit/acquisition/"
              echo "paths=$paths" >> $GITHUB_OUTPUT
              ;;
            "search-filtering-streaming")
              paths="tests/unit/search/ tests/unit/filtering/"
              paths="$paths tests/unit/streaming/ tests/unit/batch/"
              echo "paths=$paths" >> $GITHUB_OUTPUT
              ;;
            "cli-ui-reporting")
              paths="tests/unit/cli/ tests/unit/ui/"
              paths="$paths tests/unit/reporting/"
              paths="$paths tests/unit/visualization/"
              paths="$paths tests/unit/exporters/"
              echo "paths=$paths" >> $GITHUB_OUTPUT
              ;;
            "unit-workflows")
              paths="tests/unit/workflow/ tests/unit/workflows/"
              paths="$paths tests/unit/pipeline/"
              paths="$paths tests/unit/session/"
              paths="$paths tests/unit/sessions/"
              paths="$paths tests/unit/automotive/"
              paths="$paths tests/unit/integrations/"
              echo "paths=$paths" >> $GITHUB_OUTPUT
              ;;
            "unit-exploratory")
              paths="tests/unit/exploratory/ tests/unit/inference/"
              paths="$paths tests/unit/discovery/"
              paths="$paths tests/unit/guidance/"
              paths="$paths tests/unit/onboarding/"
              paths="$paths tests/unit/compliance/"
              paths="$paths tests/unit/quality/"
              paths="$paths tests/unit/testing/"
              paths="$paths tests/unit/requirements/"
              echo "paths=$paths" >> $GITHUB_OUTPUT
              ;;
            "unit-utils")
              paths="tests/unit/utils/ tests/unit/config/"
              paths="$paths tests/unit/hooks/ tests/unit/plugins/"
              paths="$paths tests/unit/api/ tests/unit/dsl/"
              paths="$paths tests/unit/extensibility/"
              paths="$paths tests/unit/jupyter/"
              paths="$paths tests/unit/optimization/"
              paths="$paths tests/unit/schemas/"
              paths="$paths tests/unit/triggering/"
              paths="$paths tests/unit/component/"
              paths="$paths tests/unit/comparison/"
              paths="$paths tests/unit/builders/"
              paths="$paths tests/unit/convenience/"
              paths="$paths tests/unit/export/"
              echo "paths=$paths" >> $GITHUB_OUTPUT
              ;;
            "non-unit-tests")
              paths="tests/integration/"
              paths="$paths tests/compliance/"
              paths="$paths tests/validation/"
              echo "paths=$paths" >> $GITHUB_OUTPUT
              ;;
          esac

      - name: Run tests with coverage (${{ matrix.test-group }})
        run: |
          # Determine optimal worker count based on test group memory requirements
          # Memory-intensive groups (large signals, real captures) use fewer workers
          if [[ "${{ matrix.test-group }}" =~ ^(analyzers|non-unit-tests)$ ]]; then
            WORKERS=2
            echo "Using 2 workers for memory-intensive test group: ${{ matrix.test-group }}"
          else
            WORKERS=4
            echo "Using 4 workers for standard test group: ${{ matrix.test-group }}"
          fi

          uv run python -m pytest ${{ steps.test-paths.outputs.paths }} \
            -v \
            -m "not slow and not performance" \
            -n $WORKERS \
            --maxprocesses=$WORKERS \
            --dist loadscope \
            --hypothesis-profile=ci \
            -p no:benchmark \
            --cov=src/oscura \
            --cov-report=xml \
            --cov-report=term-missing \
            --cov-fail-under=0 \
            --maxfail=10 \
            --tb=short \
            --durations=10 \
            --reruns 2 \
            --reruns-delay 1 \
            --junit-xml=test-results-${{ matrix.python-version }}-${{ matrix.test-group }}.xml

      - name: Upload test results
        uses: actions/upload-artifact@v6
        if: always()
        with:
          name: test-results-${{ matrix.python-version }}-${{ matrix.test-group }}
          path: test-results-${{ matrix.python-version }}-${{ matrix.test-group }}.xml
          retention-days: ${{ env.RETENTION_SHORT }}

      - name: Combine coverage data
        if: matrix.python-version == '3.12'
        run: |
          # Check if there are parallel coverage files to combine
          # pytest-cov with -n usually combines automatically, but check just in case
          if ls .coverage.* 1> /dev/null 2>&1; then
            echo "Found parallel coverage files, combining..."
            uv run coverage combine
          else
            echo "No parallel coverage files found (pytest-cov already combined)"
          fi

          # Verify .coverage file exists
          if [ -f .coverage ]; then
            ls -la .coverage
            echo "✓ Coverage file ready"
          else
            echo "✗ ERROR: No .coverage file found!"
            exit 1
          fi

      - name: Upload coverage report
        uses: actions/upload-artifact@v6
        if: matrix.python-version == '3.12'
        with:
          name: coverage-${{ matrix.test-group }}
          path: |
            .coverage
            coverage.xml
          retention-days: ${{ env.RETENTION_SHORT }}
          include-hidden-files: true

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        if: matrix.python-version == '3.12'
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: coverage.xml
          flags: unittests
          name: codecov-${{ matrix.python-version }}-${{ matrix.test-group }}
          fail_ci_if_error: false
          verbose: true

  # ============================================================================
  # Stage 3: Specialized tests (depend on basic tests)
  # ============================================================================

  diff-coverage:
    name: Diff Coverage
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [test]
    if: github.event_name == 'pull_request'
    permissions:
      contents: read
      pull-requests: write
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 0 # Need full history for diff

      - name: Setup Python Environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download coverage reports
        uses: actions/download-artifact@v7
        with:
          pattern: coverage-*
          path: coverage-reports

      - name: Merge coverage reports
        run: |
          # Install coverage tool
          uv pip install coverage[toml]

          # List downloaded artifacts for debugging
          echo "Downloaded artifacts:"
          ls -la coverage-reports/

          # Find and rename all .coverage files to avoid conflicts
          # Artifacts are in subdirectories: coverage-reports/coverage-*/
          i=0
          for dir in coverage-reports/coverage-*/; do
            if [ -f "$dir/.coverage" ]; then
              group=$(basename "$dir" | sed 's/coverage-//')
              cp "$dir/.coverage" ".coverage.$group"
              echo "Found coverage for group: $group"
              i=$((i+1))
            fi
          done

          echo "Found $i coverage files"

          # Combine all coverage database files
          if [ $i -gt 0 ]; then
            uv run coverage combine .coverage.*

            # Generate XML report from combined coverage (skip fail_under check)
            COVERAGE_CORE=sysmon uv run coverage xml -o coverage-diff.xml --fail-under=0
          else
            echo "No coverage files found!"
            exit 1
          fi

      - name: Check diff coverage
        run: |
          # Install diff-cover
          uv pip install diff-cover

          # Run diff-cover with 80% threshold for changed lines
          uv run diff-cover coverage-diff.xml \
            --compare-branch=origin/${{ github.base_ref }} \
            --fail-under=80 \
            --html-report=diff-coverage.html \
            --markdown-report=diff-coverage.md

      - name: Upload diff coverage report
        uses: actions/upload-artifact@v6
        if: always()
        with:
          name: diff-coverage-report
          path: |
            diff-coverage.html
            diff-coverage.md
          retention-days: ${{ env.RETENTION_SHORT }}

      - name: Comment PR with diff coverage
        if: always()
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');
            let diffCoverage = '';
            try {
              diffCoverage = fs.readFileSync('diff-coverage.md', 'utf8');
            } catch (err) {
              diffCoverage = 'Diff coverage report not available';
            }

            const body = `## Diff Coverage Report\n\n${diffCoverage}\n\n*Coverage must be ≥80% for changed lines*`;

            // Find existing comment
            const comments = await github.rest.issues.listComments({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
            });

            const existingComment = comments.data.find(comment =>
              comment.body.includes('## Diff Coverage Report')
            );

            if (existingComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                comment_id: existingComment.id,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: body
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: body
              });
            }

  compliance:
    name: IEEE/JEDEC Compliance
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [test]
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Setup Python Environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Verify test data (optional)
        run: ./scripts/testing/verify_test_data.sh
        continue-on-error: true

      - name: Run compliance tests
        run: |
          uv run python -m pytest tests/compliance/ \
            -v \
            -m compliance \
            --tb=short \
            --reruns 2 \
            --reruns-delay 1

  integration:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [test]
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Setup Python Environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Verify test data (optional)
        run: ./scripts/testing/verify_test_data.sh
        continue-on-error: true

      - name: Run integration tests
        run: |
          uv run python -m pytest tests/integration/ \
            -v \
            -m integration \
            -p no:benchmark \
            --maxfail=10 \
            --tb=short \
            --reruns 2 \
            --reruns-delay 1

      - name: Test CLI commands
        run: |
          uv run oscura --version
          uv run oscura --help

  # ============================================================================
  # Stage 4: Build and benchmarks (depend on all tests)
  # ============================================================================

  build:
    name: Build Package
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [pre-commit, lint, typecheck, test]
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Setup Python Environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          install-deps: "false"

      - name: Build package
        run: uv build

      - name: Verify package
        run: |
          # Check that wheel and sdist were created
          ls -la dist/
          # Verify wheel can be inspected
          python -m zipfile -l dist/*.whl | head -20

      - name: Upload artifacts
        uses: actions/upload-artifact@v6
        with:
          name: dist
          path: dist/
          retention-days: ${{ env.RETENTION_MEDIUM }}

  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [test]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Setup Python Environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Run benchmarks
        run: |
          uv run python -m pytest tests/performance/ \
            -v \
            -m "benchmark or performance" \
            --benchmark-only \
            --benchmark-json=benchmark.json
        continue-on-error: true

      - name: Store benchmark result
        uses: actions/upload-artifact@v6
        with:
          name: benchmark-results
          path: benchmark.json
          retention-days: ${{ env.RETENTION_LONG }}

  # ============================================================================
  # Final status check
  # ============================================================================

  ci-success:
    name: CI
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [pre-commit, lint, typecheck, config-validation, test, build]
    if: always()
    steps:
      - name: Check all jobs
        run: |
          echo "Pre-commit: ${{ needs.pre-commit.result }}"
          echo "Lint: ${{ needs.lint.result }}"
          echo "Typecheck: ${{ needs.typecheck.result }}"
          echo "Config Validation: ${{ needs.config-validation.result }}"
          echo "Test: ${{ needs.test.result }}"
          echo "Build: ${{ needs.build.result }}"

          # Check required jobs (allow skipped for optional jobs)
          REQUIRED_PASSED=true

          if [[ "${{ needs.pre-commit.result }}" != "success" ]]; then
            echo "::error::Pre-commit checks failed"
            REQUIRED_PASSED=false
          fi

          if [[ "${{ needs.lint.result }}" != "success" ]]; then
            echo "::error::Lint checks failed"
            REQUIRED_PASSED=false
          fi

          if [[ "${{ needs.typecheck.result }}" != "success" ]]; then
            echo "::error::Type check failed"
            REQUIRED_PASSED=false
          fi

          if [[ "${{ needs.config-validation.result }}" != "success" ]]; then
            echo "::error::Config validation failed"
            REQUIRED_PASSED=false
          fi

          if [[ "${{ needs.test.result }}" != "success" ]]; then
            echo "::error::Tests failed"
            REQUIRED_PASSED=false
          fi

          # Build can be skipped (only runs for certain branches/events)
          if [[ "${{ needs.build.result }}" == "failure" ]]; then
            echo "::error::Build failed"
            REQUIRED_PASSED=false
          fi

          if [[ "$REQUIRED_PASSED" == "false" ]]; then
            echo "Some CI checks failed"
            exit 1
          fi

          echo "✅ All CI checks passed!"
          exit 0
