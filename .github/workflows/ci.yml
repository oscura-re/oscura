name: CI

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

# Auto-cancel outdated runs for the same branch/PR
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

# Minimal permissions for security
permissions:
  contents: read
  pull-requests: read
  checks: write

env:
  PYTHON_VERSION: "3.12"
  UV_CACHE_DIR: /tmp/.uv-cache
  # Artifact retention policy (days)
  RETENTION_SHORT: 14 # Test results, temporary artifacts
  RETENTION_MEDIUM: 30 # Code quality, benchmarks
  RETENTION_LONG: 90 # Security, compliance reports

jobs:
  # ============================================================================
  # Stage 1: Fast checks (run in parallel, fail fast)
  # ============================================================================

  pre-commit:
    name: Pre-Commit Hooks
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Set up Node.js
        uses: actions/setup-node@v6
        with:
          node-version: "20"

      - name: Setup Python Environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pre-commit
        uses: actions/cache@v5
        with:
          path: ~/.cache/pre-commit
          key: pre-commit-${{ runner.os }}-${{ hashFiles('.pre-commit-config.yaml') }}
          restore-keys: |
            pre-commit-${{ runner.os }}-

      - name: Install pre-commit
        run: pip install pre-commit

      - name: Run pre-commit hooks
        run: |
          # Run pre-commit and capture exit code and output
          set +e
          PRE_COMMIT_OUTPUT=$(pre-commit run --all-files --show-diff-on-failure 2>&1)
          PRECOMMIT_EXIT=$?
          set -e

          echo "$PRE_COMMIT_OUTPUT"

          # If pre-commit made changes, check if only badges changed
          if [ $PRECOMMIT_EXIT -ne 0 ]; then
            if git diff --name-only | grep -q '^docs/badges/'; then
              echo "⚠️ Badge files were auto-updated by interrogate"
              # If ONLY badge files changed, this is OK
              if [ $(git diff --name-only | wc -l) -eq $(git diff --name-only | grep '^docs/badges/' | wc -l) ]; then
                echo "✅ Only badge files changed - allowing"
                exit 0
              fi
            fi

            # Otherwise fail
            exit $PRECOMMIT_EXIT
          fi

  lint:
    name: Lint
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Setup Python Environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Run ruff check
        run: uv run ruff check src/ tests/

      - name: Run ruff format check
        run: uv run ruff format --check src/ tests/

  typecheck:
    name: Type Check
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Setup Python Environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache mypy
        uses: actions/cache@v5
        with:
          path: .mypy_cache
          key: mypy-${{ runner.os }}-${{ env.PYTHON_VERSION }}-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            mypy-${{ runner.os }}-${{ env.PYTHON_VERSION }}-

      - name: Run mypy
        run: uv run mypy src/

  config-validation:
    name: Config Validation
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Setup Python Environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          extras: "--no-dev"

      - name: Validate orchestration config consistency
        run: uv run python .claude/hooks/validate_config_consistency.py

      - name: Validate SSOT
        run: uv run python .claude/hooks/validate_ssot.py

      - name: Run hook unit tests
        run: uv run python tests/unit/hooks/test_hooks_comprehensive.py

  # ============================================================================
  # Stage 2: Tests (depend on lint passing)
  # ============================================================================

  test:
    name: Test (Python ${{ matrix.python-version }}, ${{ matrix.test-group }})
    runs-on: ubuntu-latest
    timeout-minutes: 25
    needs: [lint] # Only needs lint - typecheck can run in parallel
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.12", "3.13"]
        test-group:
          - "analyzers-1a"
          - "analyzers-1b"
          - "analyzers-2"
          - "analyzers-3a-3d"
          - "analyzers-3b-1"
          - "analyzers-3b-2"
          - "analyzers-3c"
          - "core-protocols-loaders"
          - "unit-root-tests"
          - "cli-ui-reporting"
          - "unit-workflows"
          - "unit-discovery-inference"
          - "unit-utils"
          - "integration-tests"
          - "compliance-validation"
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Setup Python Environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ matrix.python-version }}
          dev-group: true

      - name: Cache pytest
        uses: actions/cache@v5
        with:
          path: .pytest_cache
          key: pytest-${{ runner.os }}-${{ matrix.python-version }}-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            pytest-${{ runner.os }}-${{ matrix.python-version }}-

      - name: Cache Hypothesis examples
        uses: actions/cache@v5
        with:
          path: .hypothesis
          key: hypothesis-${{ runner.os }}-${{ matrix.python-version }}-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            hypothesis-${{ runner.os }}-${{ matrix.python-version }}-
            hypothesis-${{ runner.os }}-

      - name: Cache Numba JIT
        uses: actions/cache@v5
        with:
          path: ~/.numba
          key: numba-${{ runner.os }}-py${{ matrix.python-version }}-${{ hashFiles('src/**/*.py') }}
          restore-keys: |
            numba-${{ runner.os }}-py${{ matrix.python-version }}-

      - name: Verify test data (optional)
        run: ./scripts/testing/verify_test_data.sh
        continue-on-error: true

      - name: Determine test paths
        id: test-paths
        run: |
          case "${{ matrix.test-group }}" in
            "analyzers-1a")
              # Protocols analyzers (~16K lines, ~8-10 min)
              paths="tests/unit/analyzers/protocols/"
              echo "paths=$paths" >> $GITHUB_OUTPUT
              ;;
            "analyzers-1b")
              # Digital, waveform, eye, jitter analyzers (~15K lines, ~8-10 min)
              paths="tests/unit/analyzers/digital/ tests/unit/analyzers/waveform/"
              paths="$paths tests/unit/analyzers/eye/ tests/unit/analyzers/jitter/"
              echo "paths=$paths" >> $GITHUB_OUTPUT
              ;;
            "analyzers-2")
              # Spectral, power, patterns, statistical analyzers (~1800 tests, ~8-10 min)
              paths="tests/unit/analyzers/spectral/ tests/unit/analyzers/power/"
              paths="$paths tests/unit/analyzers/patterns/ tests/unit/analyzers/statistical/"
              paths="$paths tests/unit/analyzers/statistics/"
              echo "paths=$paths" >> $GITHUB_OUTPUT
              ;;
            "analyzers-3a-3d")
              # ML, side_channel, and signal_integrity analyzers (merged small groups)
              paths="tests/unit/analyzers/ml/ tests/unit/analyzers/side_channel/"
              paths="$paths tests/unit/analyzers/signal_integrity/"
              echo "paths=$paths" >> $GITHUB_OUTPUT
              ;;
            "analyzers-3b-1")
              # Packet analyzer batch 1 - parser, stream, metrics (139 tests, ~13 min)
              paths="tests/unit/analyzers/packet/test_parser.py"
              paths="$paths tests/unit/analyzers/packet/test_stream.py"
              paths="$paths tests/unit/analyzers/packet/test_metrics.py"
              echo "paths=$paths" >> $GITHUB_OUTPUT
              ;;
            "analyzers-3b-2")
              # Packet analyzer batch 2 - daq, payload, extraction (145 tests, ~13 min)
              paths="tests/unit/analyzers/packet/test_daq.py"
              paths="$paths tests/unit/analyzers/packet/test_payload.py"
              paths="$paths tests/unit/analyzers/packet/test_payload_extraction.py"
              echo "paths=$paths" >> $GITHUB_OUTPUT
              ;;
            "analyzers-3c")
              # Root analyzer tests + analysis/signal/correlation
              paths="tests/unit/analyzers/test_*.py tests/unit/analysis/"
              paths="$paths tests/unit/signal/ tests/unit/correlation/"
              echo "paths=$paths" >> $GITHUB_OUTPUT
              ;;
            "core-protocols-loaders")
              # Core functionality, protocol decoders, data loaders
              paths="tests/unit/core/ tests/unit/protocols/ tests/unit/loaders/"
              echo "paths=$paths" >> $GITHUB_OUTPUT
              ;;
            "unit-root-tests")
              # Standalone test files in tests/unit/ root only
              paths="tests/unit/test_*.py"
              echo "paths=$paths" >> $GITHUB_OUTPUT
              ;;
            "cli-ui-reporting")
              # CLI, reporting, visualization, web interface (excluding empty: ui, onboarding)
              paths="tests/unit/cli/ tests/unit/reporting/"
              paths="$paths tests/unit/visualization/ tests/unit/web/"
              echo "paths=$paths" >> $GITHUB_OUTPUT
              ;;
            "unit-workflows")
              # Workflows, sessions, automotive, hardware/IoT (excluding empty: pipeline, batch, workflow, session)
              paths="tests/unit/workflows/ tests/unit/sessions/"
              paths="$paths tests/unit/automotive/ tests/unit/hardware/"
              paths="$paths tests/unit/firmware/ tests/unit/iot/"
              paths="$paths tests/unit/side_channel/"
              echo "paths=$paths" >> $GITHUB_OUTPUT
              ;;
            "unit-discovery-inference")
              # Inference, discovery, guidance, compliance, requirements
              # (excluding empty: exploratory, dsl, extensibility)
              paths="tests/unit/inference/ tests/unit/discovery/"
              paths="$paths tests/unit/guidance/ tests/unit/compliance/"
              paths="$paths tests/unit/requirements/"
              echo "paths=$paths" >> $GITHUB_OUTPUT
              ;;
            "unit-utils")
              # Utils, config, API, helpers, exports
              # (excluding empty: builders, integrations, plugins, quality, schemas,
              #  exporters, testing, triggering, acquisition, math, optimization)
              paths="tests/unit/utils/ tests/unit/config/ tests/unit/hooks/"
              paths="$paths tests/unit/api/ tests/unit/jupyter/"
              paths="$paths tests/unit/component/ tests/unit/comparison/"
              paths="$paths tests/unit/convenience/ tests/unit/export/"
              paths="$paths tests/unit/storage/ tests/unit/validation/"
              paths="$paths tests/unit/security/ tests/unit/performance/"
              echo "paths=$paths" >> $GITHUB_OUTPUT
              ;;
            "integration-tests")
              # Integration tests (144 tests, ~9-10min)
              # Removed misplaced unit tests (test_config_driven, test_database_workflows, test_export_roundtrips)
              # test_examples.py excluded (documentation validation, in nightly CI)
              paths="tests/integration/test_binary_to_protocol.py"
              paths="$paths tests/integration/test_complete_workflows.py"
              paths="$paths tests/integration/test_complex_scenarios.py"
              paths="$paths tests/integration/test_integration_workflows.py"
              paths="$paths tests/integration/test_multi_format_pipelines.py"
              paths="$paths tests/integration/test_pcap_to_inference.py"
              paths="$paths tests/integration/test_protocol_analysis_workflows.py"
              paths="$paths tests/integration/test_wfm_loading.py"
              paths="$paths tests/integration/reporting/"
              echo "paths=$paths" >> $GITHUB_OUTPUT
              ;;
            "compliance-validation")
              # Compliance and validation tests (262 tests, ~9-10min)
              paths="tests/compliance/ tests/validation/"
              echo "paths=$paths" >> $GITHUB_OUTPUT
              ;;
          esac

          # Validate that test paths exist (fail fast if misconfigured)
          echo "Validating test paths..."
          for path in $paths; do
            if [[ ! -e "$path" ]]; then
              echo "ERROR: Test path does not exist: $path"
              echo "This indicates a CI configuration error in test-group: ${{ matrix.test-group }}"
              exit 1
            fi
          done
          echo "✓ All test paths validated for ${{ matrix.test-group }}"

      - name: Run tests with coverage (${{ matrix.test-group }})
        id: run-tests
        env:
          NUMBA_CACHE_DIR: ~/.numba
          COVERAGE_CORE: sysmon
          HYPOTHESIS_PROFILE: ci
        run: |
          # Record start time
          START_TIME=$(date +%s)
          echo "start_time=$START_TIME" >> $GITHUB_OUTPUT

          # Determine optimal worker count based on test group memory requirements
          # Memory-intensive groups (analyzers-*, integration-tests, compliance-validation): Use 2 workers
          # Standard groups: Use 4 workers
          if [[ "${{ matrix.test-group }}" =~ ^(analyzers-|integration-tests|compliance-validation) ]]; then
            WORKERS=2
            XDIST_ARGS="-n $WORKERS --maxprocesses=$WORKERS --dist loadscope"
            echo "Using 2 workers for memory-intensive test group: ${{ matrix.test-group }}"
          else
            WORKERS=4
            XDIST_ARGS="-n $WORKERS --maxprocesses=$WORKERS --dist loadscope"
            echo "Using 4 workers for standard test group: ${{ matrix.test-group }}"
          fi

          # Run tests and capture exit code
          set +e
          uv run python -m pytest ${{ steps.test-paths.outputs.paths }} \
            -v \
            -m "not slow and not performance" \
            $XDIST_ARGS \
            --hypothesis-profile=ci \
            -p no:benchmark \
            --cov=oscura \
            --cov-report=xml \
            --cov-report=term-missing \
            --cov-fail-under=0 \
            --maxfail=10 \
            --tb=short \
            --durations=20 \
            --reruns 2 \
            --reruns-delay 1 \
            --junit-xml=test-results-${{ matrix.python-version }}-${{ matrix.test-group }}.xml
          TEST_EXIT_CODE=$?
          set -e

          # Always record end time and duration
          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          echo "duration_seconds=$DURATION" >> $GITHUB_OUTPUT
          echo "Test execution took ${DURATION}s ($((DURATION / 60))m $((DURATION % 60))s)"

          # Exit with test result
          exit $TEST_EXIT_CODE

      - name: Check test duration
        if: always()
        run: |
          DURATION="${{ steps.run-tests.outputs.duration_seconds }}"
          TIMEOUT_SECONDS=$((25 * 60))  # 25 minutes in seconds
          WARNING_THRESHOLD=$((TIMEOUT_SECONDS * 70 / 100))  # 70% of timeout (1050s / 17.5m)
          ERROR_THRESHOLD=$((TIMEOUT_SECONDS * 85 / 100))    # 85% of timeout (1275s / 21.25m)

          # Handle case where duration wasn't recorded (shouldn't happen but be safe)
          if [ -z "$DURATION" ]; then
            echo "Duration not recorded (step may have been skipped)"
            exit 0
          fi

          # Fail build if exceeding 85% threshold
          if [ "$DURATION" -gt "$ERROR_THRESHOLD" ]; then
            MIN=$((DURATION / 60))
            SEC=$((DURATION % 60))
            echo "::error::Test took ${DURATION}s (${MIN}m ${SEC}s), exceeding 85% of 25m timeout. MUST split!"
            echo "Duration: ${DURATION}s ($((DURATION / 60))m $((DURATION % 60))s)"
            echo "Warning threshold (70%): ${WARNING_THRESHOLD}s ($((WARNING_THRESHOLD / 60))m)"
            echo "Error threshold (85%): ${ERROR_THRESHOLD}s ($((ERROR_THRESHOLD / 60))m)"
            echo "Timeout: ${TIMEOUT_SECONDS}s (25m)"
            exit 1
          fi

          # Warning if exceeding 70% threshold
          if [ "$DURATION" -gt "$WARNING_THRESHOLD" ]; then
            MIN=$((DURATION / 60))
            SEC=$((DURATION % 60))
            echo "::warning::Test took ${DURATION}s (${MIN}m ${SEC}s), approaching 25m timeout. Split?"
          fi

          echo "Duration: ${DURATION}s ($((DURATION / 60))m $((DURATION % 60))s)"
          echo "Warning threshold (70%): ${WARNING_THRESHOLD}s ($((WARNING_THRESHOLD / 60))m)"
          echo "Error threshold (85%): ${ERROR_THRESHOLD}s ($((ERROR_THRESHOLD / 60))m)"
          echo "Timeout: ${TIMEOUT_SECONDS}s (25m)"

      - name: Upload test results
        uses: actions/upload-artifact@v6
        if: always()
        with:
          name: test-results-${{ matrix.python-version }}-${{ matrix.test-group }}
          path: test-results-${{ matrix.python-version }}-${{ matrix.test-group }}.xml
          retention-days: ${{ env.RETENTION_SHORT }}

      - name: Create test duration report
        if: always()
        run: |
          # Create JSON report with duration metrics
          DURATION=${{ steps.run-tests.outputs.duration_seconds || 0 }}
          cat > test-duration-${{ matrix.python-version }}-${{ matrix.test-group }}.json <<EOF
          {
            "test_group": "${{ matrix.test-group }}",
            "python_version": "${{ matrix.python-version }}",
            "duration_seconds": ${DURATION},
            "duration_minutes": $(echo "scale=2; ${DURATION} / 60" | bc -l),
            "timeout_seconds": $((25 * 60)),
            "warning_threshold_percent": 70,
            "error_threshold_percent": 85,
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          }
          EOF
          cat test-duration-${{ matrix.python-version }}-${{ matrix.test-group }}.json

      - name: Upload test duration metrics
        uses: actions/upload-artifact@v6
        if: always()
        with:
          name: test-duration-${{ matrix.python-version }}-${{ matrix.test-group }}
          path: test-duration-${{ matrix.python-version }}-${{ matrix.test-group }}.json
          retention-days: ${{ env.RETENTION_MEDIUM }}

      - name: Combine coverage data
        if: matrix.python-version == '3.12'
        run: |
          # Check if there are parallel coverage files to combine
          # pytest-cov with -n usually combines automatically, but check just in case
          if ls .coverage.* 1> /dev/null 2>&1; then
            echo "Found parallel coverage files, combining..."
            uv run coverage combine
          else
            echo "No parallel coverage files found (pytest-cov already combined)"
          fi

          # Verify .coverage file exists
          if [ -f .coverage ]; then
            ls -la .coverage
            echo "✓ Coverage file ready"
          else
            echo "✗ ERROR: No .coverage file found!"
            exit 1
          fi

      - name: Upload coverage report
        uses: actions/upload-artifact@v6
        if: matrix.python-version == '3.12'
        with:
          name: coverage-${{ matrix.test-group }}
          path: |
            .coverage
            coverage.xml
          retention-days: ${{ env.RETENTION_SHORT }}
          include-hidden-files: true

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        if: matrix.python-version == '3.12'
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: coverage.xml
          flags: unittests
          name: codecov-${{ matrix.python-version }}-${{ matrix.test-group }}
          fail_ci_if_error: false
          verbose: true

  diff-coverage:
    name: Diff Coverage
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [test]
    if: github.event_name == 'pull_request'
    permissions:
      contents: read
      pull-requests: write
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 0 # Need full history for diff

      - name: Setup Python Environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download coverage reports
        uses: actions/download-artifact@v7
        with:
          pattern: coverage-*
          path: coverage-reports

      - name: Merge coverage reports
        run: |
          # Install coverage tool
          uv pip install coverage[toml]

          # List downloaded artifacts for debugging
          echo "Downloaded artifacts:"
          ls -la coverage-reports/

          # Find and rename all .coverage files to avoid conflicts
          # Artifacts are in subdirectories: coverage-reports/coverage-*/
          i=0
          for dir in coverage-reports/coverage-*/; do
            if [ -f "$dir/.coverage" ]; then
              group=$(basename "$dir" | sed 's/coverage-//')
              cp "$dir/.coverage" ".coverage.$group"
              echo "Found coverage for group: $group"
              i=$((i+1))
            fi
          done

          echo "Found $i coverage files"

          # Combine all coverage database files
          if [ $i -gt 0 ]; then
            uv run coverage combine .coverage.*

            # Generate XML report from combined coverage (skip fail_under check)
            COVERAGE_CORE=sysmon uv run coverage xml -o coverage-diff.xml --fail-under=0
          else
            echo "No coverage files found!"
            exit 1
          fi

      - name: Check diff coverage
        run: |
          # Install diff-cover
          uv pip install diff-cover

          # Run diff-cover with 80% threshold for changed lines
          uv run diff-cover coverage-diff.xml \
            --compare-branch=origin/${{ github.base_ref }} \
            --fail-under=80 \
            --html-report=diff-coverage.html \
            --markdown-report=diff-coverage.md

      - name: Upload diff coverage report
        uses: actions/upload-artifact@v6
        if: always()
        with:
          name: diff-coverage-report
          path: |
            diff-coverage.html
            diff-coverage.md
          retention-days: ${{ env.RETENTION_SHORT }}

      - name: Comment PR with diff coverage
        if: always()
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');
            let diffCoverage = '';
            try {
              diffCoverage = fs.readFileSync('diff-coverage.md', 'utf8');
            } catch (err) {
              diffCoverage = 'Diff coverage report not available';
            }

            const body = `## Diff Coverage Report\n\n${diffCoverage}\n\n*Coverage must be ≥80% for changed lines*`;

            // Find existing comment
            const comments = await github.rest.issues.listComments({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
            });

            const existingComment = comments.data.find(comment =>
              comment.body.includes('## Diff Coverage Report')
            );

            if (existingComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                comment_id: existingComment.id,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: body
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: body
              });
            }

  compliance:
    name: IEEE/JEDEC Compliance
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [test]
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Setup Python Environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Verify test data (optional)
        run: ./scripts/testing/verify_test_data.sh
        continue-on-error: true

      - name: Run compliance tests
        run: |
          uv run python -m pytest tests/compliance/ \
            -v \
            -m compliance \
            --tb=short \
            --reruns 2 \
            --reruns-delay 1

  integration:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [test]
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Setup Python Environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Verify test data (optional)
        run: ./scripts/testing/verify_test_data.sh
        continue-on-error: true

      - name: Run integration tests
        run: |
          uv run python -m pytest tests/integration/ \
            -v \
            -m integration \
            -p no:benchmark \
            --maxfail=10 \
            --tb=short \
            --reruns 2 \
            --reruns-delay 1

      - name: Test CLI commands
        run: |
          uv run oscura --version
          uv run oscura --help

  # ============================================================================
  # Stage 4: Build and benchmarks (depend on all tests)
  # ============================================================================

  build:
    name: Build Package
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [pre-commit, lint, typecheck, test]
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Setup Python Environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          install-deps: "false"

      - name: Build package
        run: uv build

      - name: Verify package
        run: |
          # Check that wheel and sdist were created
          ls -la dist/
          # Verify wheel can be inspected
          python -m zipfile -l dist/*.whl | head -20

      - name: Upload artifacts
        uses: actions/upload-artifact@v6
        with:
          name: dist
          path: dist/
          retention-days: ${{ env.RETENTION_MEDIUM }}

  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [test]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Setup Python Environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Run benchmarks
        run: |
          uv run python -m pytest tests/performance/ \
            -v \
            -m "benchmark or performance" \
            --benchmark-only \
            --benchmark-json=benchmark.json
        continue-on-error: true

      - name: Store benchmark result
        uses: actions/upload-artifact@v6
        with:
          name: benchmark-results
          path: benchmark.json
          retention-days: ${{ env.RETENTION_LONG }}

  # ============================================================================
  # Final status check
  # ============================================================================

  ci-success:
    name: CI
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [pre-commit, lint, typecheck, config-validation, test, build]
    if: always()
    steps:
      - name: Check all jobs
        run: |
          echo "Pre-commit: ${{ needs.pre-commit.result }}"
          echo "Lint: ${{ needs.lint.result }}"
          echo "Typecheck: ${{ needs.typecheck.result }}"
          echo "Config Validation: ${{ needs.config-validation.result }}"
          echo "Test: ${{ needs.test.result }}"
          echo "Build: ${{ needs.build.result }}"

          # Check required jobs (allow skipped for optional jobs)
          REQUIRED_PASSED=true

          if [[ "${{ needs.pre-commit.result }}" != "success" ]]; then
            echo "::error::Pre-commit checks failed"
            REQUIRED_PASSED=false
          fi

          if [[ "${{ needs.lint.result }}" != "success" ]]; then
            echo "::error::Lint checks failed"
            REQUIRED_PASSED=false
          fi

          if [[ "${{ needs.typecheck.result }}" != "success" ]]; then
            echo "::error::Type check failed"
            REQUIRED_PASSED=false
          fi

          if [[ "${{ needs.config-validation.result }}" != "success" ]]; then
            echo "::error::Config validation failed"
            REQUIRED_PASSED=false
          fi

          if [[ "${{ needs.test.result }}" != "success" ]]; then
            echo "::error::Tests failed"
            REQUIRED_PASSED=false
          fi


          # Build can be skipped (only runs for certain branches/events)
          if [[ "${{ needs.build.result }}" == "failure" ]]; then
            echo "::error::Build failed"
            REQUIRED_PASSED=false
          fi

          if [[ "$REQUIRED_PASSED" == "false" ]]; then
            echo "Some CI checks failed"
            exit 1
          fi

          echo "✅ All CI checks passed!"
          exit 0
