"""Integration tests for complex real-world scenarios.

Tests end-to-end complex scenarios combining multiple workflows:
- Multi-protocol capture analysis with cross-correlation
- Long-running capture processing with resource management
- Failure recovery and partial results preservation
- Real-world data quality issues (noise, corruption, gaps)
- Performance benchmarks for production-scale data

Requirements: Tests realistic complex scenarios with multiple interacting components.
"""

from __future__ import annotations

from pathlib import Path
from typing import Literal, cast

import numpy as np
import pytest

# Graceful imports
try:
    from oscura.analyzers.digital.clock import ClockRecovery
    from oscura.analyzers.digital.edges import EdgeDetector
    from oscura.analyzers.statistical.entropy import EntropyAnalyzer
    from oscura.core.types import DigitalTrace, TraceMetadata
    from oscura.loaders.configurable import (
        ConfigurablePacketLoader,
        PacketFormatConfig,
        SampleFormatDef,
    )
    from oscura.validation.testing.synthetic import generate_digital_signal, generate_packets

    IMPORTS_AVAILABLE = True
except ImportError as e:
    IMPORTS_AVAILABLE = False
    IMPORT_ERROR = str(e)

pytestmark = [pytest.mark.integration, pytest.mark.workflow]


@pytest.mark.integration
class TestMultiProtocolCorrelation:
    """Test complex scenarios with multiple protocols."""

    def test_uart_spi_timing_correlation(self, tmp_path: Path) -> None:
        """Test correlating timing between UART and SPI signals.

        Workflow:
        1. Generate UART signal on channel 1
        2. Generate SPI signals on channels 2-4 (CLK, MOSI, MISO)
        3. Detect timing events in both protocols
        4. Correlate events by timestamp
        5. Verify timing relationships
        """
        try:
            from oscura.validation.testing.synthetic import SyntheticSignalConfig

            # Generate UART signal
            uart_config = SyntheticSignalConfig(
                pattern_type="uart",
                sample_rate=10e6,
                duration_samples=100000,
            )
            uart_signal, uart_truth = generate_digital_signal(
                pattern="uart", **uart_config.__dict__
            )

            # Generate SPI clock signal (simultaneous)
            spi_config = SyntheticSignalConfig(
                pattern_type="square",
                sample_rate=10e6,
                duration_samples=100000,
            )
            spi_clk, _ = generate_digital_signal(pattern="square", **spi_config.__dict__)

            # Create traces
            metadata = TraceMetadata(sample_rate=10e6)
            uart_trace = DigitalTrace(data=uart_signal > 1.5, metadata=metadata)
            spi_trace = DigitalTrace(data=spi_clk > 1.5, metadata=metadata)

            # Detect edges in both
            detector = EdgeDetector()
            uart_rising, uart_falling = detector.detect_all_edges(
                uart_trace.data.astype(np.float64)
            )
            spi_rising, spi_falling = detector.detect_all_edges(spi_trace.data.astype(np.float64))

            # Verify both have detectable events
            assert len(uart_rising) + len(uart_falling) > 0
            assert len(spi_rising) + len(spi_falling) > 0

            # Find temporal correlations (edges within same time windows)
            if len(uart_rising) > 0 and len(spi_rising) > 0:
                # Check if any UART events occur near SPI edges (Â±100 samples)
                window = 100
                correlations = 0
                for uart_edge in uart_rising[:10]:
                    for spi_edge in spi_rising[:10]:
                        if abs(uart_edge - spi_edge) < window:
                            correlations += 1

                # May or may not find correlations depending on patterns
                assert correlations >= 0

        except (ImportError, AttributeError):
            pytest.skip("Signal generation not available")

    def test_can_lin_interleaved_decode(self, tmp_path: Path) -> None:
        """Test decoding interleaved CAN and LIN messages.

        Workflow:
        1. Generate CAN frames
        2. Generate LIN frames
        3. Interleave into single capture
        4. Decode both protocols simultaneously
        5. Verify frame counts and integrity
        """
        # Generate CAN-like packets
        can_data, can_truth = generate_packets(count=50, packet_size=16)

        # Generate LIN-like packets (smaller)
        lin_data, lin_truth = generate_packets(count=30, packet_size=8)

        # Interleave (simple alternation for testing)
        can_packets = [can_data[i : i + 16] for i in range(0, len(can_data), 16)]
        lin_packets = [lin_data[i : i + 8] for i in range(0, len(lin_data), 8)]

        interleaved = []
        for i in range(max(len(can_packets), len(lin_packets))):
            if i < len(can_packets):
                interleaved.append(can_packets[i])
            if i < len(lin_packets):
                interleaved.append(lin_packets[i])

        combined_data = b"".join(interleaved)

        # Save combined capture
        capture_file = tmp_path / "can_lin_interleaved.bin"
        capture_file.write_bytes(combined_data)

        # Try to load and separate protocols
        # In real implementation, would use protocol detection
        total_bytes = len(combined_data)
        assert total_bytes == len(can_data) + len(lin_data)

        # Verify we can identify different packet sizes
        sizes_seen = set()
        for packet in interleaved[:20]:
            sizes_seen.add(len(packet))

        # Should see both 16-byte (CAN) and 8-byte (LIN) packets
        assert 8 in sizes_seen or 16 in sizes_seen


@pytest.mark.integration
class TestResourceManagement:
    """Test resource management in long-running workflows."""

    @pytest.mark.slow
    def test_large_capture_streaming_analysis(self, tmp_path: Path) -> None:
        """Test analyzing large capture with streaming to avoid OOM.

        Workflow:
        1. Generate large capture (100MB+)
        2. Stream-load in chunks
        3. Analyze each chunk
        4. Aggregate results
        5. Verify memory stays bounded
        """
        # Generate large dataset (10,000 packets)
        large_data, _ = generate_packets(count=10000, packet_size=256)

        large_file = tmp_path / "large_capture.bin"
        large_file.write_bytes(large_data)

        # Verify file size
        file_size_mb = large_file.stat().st_size / (1024 * 1024)
        assert file_size_mb > 2.0  # At least 2MB

        # Stream-load and analyze
        loader_config = PacketFormatConfig(
            name="large_test",
            version="1.0",
            packet_size=256,
            byte_order="little",
            header_size=16,
            header_fields=[],
            sample_offset=16,
            sample_count=30,
            sample_format=SampleFormatDef(size=8, type="uint64", endian="little"),
        )

        loader = ConfigurablePacketLoader(loader_config)
        entropy_analyzer = EntropyAnalyzer()

        chunk_count = 0
        total_packets = 0
        chunk_entropies = []

        # Process in manageable chunks
        for chunk in loader.stream(large_file, chunk_size=500):
            chunk_count += 1
            total_packets += len(chunk.packets)

            # Analyze chunk - extract sample data for entropy calculation
            chunk_bytes = bytearray()
            for packet in chunk.packets[:10]:  # Sample first 10
                # Extract samples as bytes for entropy analysis
                if "samples" in packet and len(packet["samples"]) > 0:
                    # Convert sample values to bytes (uint64 = 8 bytes each)
                    for sample in packet["samples"][:5]:  # First 5 samples
                        chunk_bytes.extend(sample.to_bytes(8, byteorder="little"))

            if len(chunk_bytes) > 0:
                entropy = entropy_analyzer.calculate_entropy(bytes(chunk_bytes))
                chunk_entropies.append(entropy)

        # Verify all data processed
        assert total_packets == 10000
        assert chunk_count == 20  # 10000 / 500 = 20 chunks
        assert len(chunk_entropies) >= 10  # At least 10 chunks analyzed

    def test_incremental_analysis_with_checkpointing(self, tmp_path: Path) -> None:
        """Test incremental analysis with checkpoint/resume capability.

        Workflow:
        1. Start analysis of large dataset
        2. Process 50% of data
        3. Save checkpoint
        4. Simulate interruption
        5. Resume from checkpoint
        6. Complete analysis
        7. Verify results complete
        """
        # Generate dataset
        data, truth = generate_packets(count=1000, packet_size=128)

        data_file = tmp_path / "checkpoint_test.bin"
        data_file.write_bytes(data)

        checkpoint_file = tmp_path / "analysis.checkpoint"

        loader_config = PacketFormatConfig(
            name="checkpoint_test",
            version="1.0",
            packet_size=128,
            byte_order="little",
            header_size=16,
            header_fields=[],
            sample_offset=16,
            sample_count=14,
            sample_format=SampleFormatDef(size=8, type="uint64", endian="little"),
        )

        loader = ConfigurablePacketLoader(loader_config)

        # Phase 1: Process first half
        processed_count = 0
        for chunk in loader.stream(data_file, chunk_size=100):
            processed_count += len(chunk.packets)
            if processed_count >= 500:
                # Save checkpoint
                checkpoint_data = {
                    "processed_count": processed_count,
                    "last_position": processed_count * 128,
                }
                import json

                checkpoint_file.write_text(json.dumps(checkpoint_data))
                break

        # Verify checkpoint saved
        assert checkpoint_file.exists()
        checkpoint = json.loads(checkpoint_file.read_text())
        assert checkpoint["processed_count"] >= 500

        # Phase 2: Resume from checkpoint (in real implementation)
        # Would seek to checkpoint position and continue
        remaining_count = 0
        for chunk in loader.stream(data_file, chunk_size=100):
            remaining_count += len(chunk.packets)

        # Verify we can process full dataset
        assert remaining_count == 1000


@pytest.mark.integration
class TestFailureRecovery:
    """Test failure recovery and partial result preservation."""

    def test_partial_decode_with_corruption(self, tmp_path: Path) -> None:
        """Test recovering partial results when data is corrupted.

        Workflow:
        1. Generate valid packet sequence
        2. Inject corruption in middle
        3. Attempt decode
        4. Verify partial results before corruption preserved
        5. Verify error reported for corrupted region
        """
        # Generate valid packets
        valid_data, _ = generate_packets(count=100, packet_size=64)

        # Inject corruption at position 50 (middle)
        data_array = bytearray(valid_data)
        corruption_start = 50 * 64
        corruption_end = corruption_start + 320  # Corrupt 5 packets
        for i in range(corruption_start, min(corruption_end, len(data_array))):
            data_array[i] = 0xFF  # All 1s

        corrupted_data = bytes(data_array)

        corrupt_file = tmp_path / "corrupted.bin"
        corrupt_file.write_bytes(corrupted_data)

        # Try to load
        loader_config = PacketFormatConfig(
            name="corruption_test",
            version="1.0",
            packet_size=64,
            byte_order="little",
            header_size=8,
            header_fields=[],
            sample_offset=8,
            sample_count=7,
            sample_format=SampleFormatDef(size=8, type="uint64", endian="little"),
        )

        loader = ConfigurablePacketLoader(loader_config)

        try:
            result = loader.load(corrupt_file)

            # Should load some packets (at least the uncorrupted ones)
            # Exact count depends on error handling implementation
            assert len(result.packets) > 0

            # If loaded 100 packets, corruption was tolerated
            # If loaded < 100, partial results were preserved
            assert len(result.packets) <= 100

        except (ValueError, RuntimeError) as e:
            # Acceptable to fail with corrupted data
            pass

    def test_missing_data_gap_handling(self, tmp_path: Path) -> None:
        """Test handling missing data gaps in capture.

        Workflow:
        1. Generate packet sequence with gaps
        2. Load with gap detection
        3. Verify gaps identified
        4. Verify data on both sides of gap preserved
        """
        # Generate packets before gap
        before_gap, _ = generate_packets(count=50, packet_size=64)

        # Generate packets after gap
        after_gap, _ = generate_packets(count=50, packet_size=64)

        # Create data with gap (missing packets)
        gapped_data = before_gap + after_gap

        gap_file = tmp_path / "gapped.bin"
        gap_file.write_bytes(gapped_data)

        # Load and verify
        loader_config = PacketFormatConfig(
            name="gap_test",
            version="1.0",
            packet_size=64,
            byte_order="little",
            header_size=8,
            header_fields=[],
            sample_offset=8,
            sample_count=7,
            sample_format=SampleFormatDef(size=8, type="uint64", endian="little"),
        )

        loader = ConfigurablePacketLoader(loader_config)
        result = loader.load(gap_file)

        # Should load 100 packets (50 + 50)
        assert len(result.packets) == 100

        # In real implementation, could detect sequence number gaps
        # or timestamp discontinuities


@pytest.mark.integration
class TestDataQualityIssues:
    """Test handling real-world data quality issues."""

    def test_noisy_signal_recovery(self) -> None:
        """Test recovering signal from noisy capture.

        Workflow:
        1. Generate clean digital signal
        2. Add noise (SNR = 20 dB)
        3. Apply noise filtering
        4. Recover digital edges
        5. Verify edge detection still works
        """
        try:
            from oscura.validation.testing.synthetic import SyntheticSignalConfig

            # Generate signal with noise
            config = SyntheticSignalConfig(
                pattern_type="square",
                sample_rate=10e6,
                duration_samples=10000,
                noise_snr_db=20,  # Moderate noise
            )

            noisy_signal, truth = generate_digital_signal(pattern="square", **config.__dict__)

            # Create trace
            metadata = TraceMetadata(sample_rate=10e6)

            # Apply simple threshold (should handle moderate noise)
            # Use adaptive threshold: mean + 0.5*std
            threshold = np.mean(noisy_signal) + 0.5 * np.std(noisy_signal)
            digital = noisy_signal > threshold

            trace = DigitalTrace(data=digital, metadata=metadata)

            # Detect edges
            detector = EdgeDetector()
            rising, falling = detector.detect_all_edges(trace.data.astype(np.float64))

            # Should still detect edges despite noise
            assert len(rising) + len(falling) > 0

            # Verify clock recovery works
            recovery = ClockRecovery()
            freq = recovery.detect_frequency(trace)
            assert freq > 0

        except (ImportError, AttributeError):
            pytest.skip("Signal generation not available")

    def test_glitch_filtering(self) -> None:
        """Test filtering glitches from digital signal.

        Workflow:
        1. Generate clean digital signal
        2. Inject short glitches (< 2 samples)
        3. Apply glitch filter
        4. Verify glitches removed
        5. Verify real transitions preserved
        """
        try:
            from oscura.validation.testing.synthetic import SyntheticSignalConfig

            # Generate clean signal
            config = SyntheticSignalConfig(
                pattern_type="uart",
                sample_rate=10e6,
                duration_samples=10000,
            )

            signal, _ = generate_digital_signal(pattern="uart", **config.__dict__)

            # Inject glitches (single-sample spikes)
            rng = np.random.default_rng(42)
            glitch_positions = rng.choice(len(signal), size=50, replace=False)
            signal_with_glitches = signal.copy()
            for pos in glitch_positions:
                # Flip value for one sample
                signal_with_glitches[pos] = 1.0 - signal_with_glitches[pos]

            # Convert to digital
            digital = signal_with_glitches > 0.5

            # Apply simple glitch filter (majority vote over 3 samples)
            filtered = digital.copy()
            for i in range(1, len(digital) - 1):
                # If middle sample differs from both neighbors, it's a glitch
                if digital[i] != digital[i - 1] and digital[i] != digital[i + 1]:
                    # Replace with neighbor value
                    filtered[i] = digital[i - 1]

            # Count transitions
            original_transitions = np.sum(np.diff(digital.astype(int)) != 0)
            filtered_transitions = np.sum(np.diff(filtered.astype(int)) != 0)

            # Filtered should have fewer transitions (glitches removed)
            assert filtered_transitions <= original_transitions

        except (ImportError, AttributeError):
            pytest.skip("Signal generation not available")


@pytest.mark.integration
class TestProductionScalePerformance:
    """Test performance at production scale."""

    @pytest.mark.slow
    def test_million_packet_throughput(self, tmp_path: Path) -> None:
        """Test throughput when processing million+ packets.

        Workflow:
        1. Generate 1M packets
        2. Process with streaming
        3. Measure packets/second throughput
        4. Verify reasonable performance
        """
        # Note: Generate smaller dataset for CI, scale up for benchmarking
        packet_count = 100000  # 100K for CI, 1M for full benchmark

        # Generate in chunks to avoid memory issues during generation
        chunk_size = 10000
        all_data = bytearray()

        for i in range(packet_count // chunk_size):
            chunk_data, _ = generate_packets(count=chunk_size, packet_size=64)
            all_data.extend(chunk_data)

        large_file = tmp_path / "million_packets.bin"
        large_file.write_bytes(bytes(all_data))

        # Load and process
        loader_config = PacketFormatConfig(
            name="perf_test",
            version="1.0",
            packet_size=64,
            byte_order="little",
            header_size=8,
            header_fields=[],
            sample_offset=8,
            sample_count=7,
            sample_format=SampleFormatDef(size=8, type="uint64", endian="little"),
        )

        loader = ConfigurablePacketLoader(loader_config)

        import time

        start_time = time.time()
        processed_count = 0

        for chunk in loader.stream(large_file, chunk_size=5000):
            processed_count += len(chunk.packets)

        elapsed = time.time() - start_time

        # Verify all processed
        assert processed_count == packet_count

        # Calculate throughput
        packets_per_sec = processed_count / elapsed if elapsed > 0 else 0

        # Should process at least 10K packets/sec (very conservative)
        # Production systems should achieve 100K+ packets/sec
        assert packets_per_sec > 1000  # Minimum viable throughput

    @pytest.mark.slow
    def test_concurrent_protocol_decode(self, tmp_path: Path) -> None:
        """Test concurrent decoding of multiple protocol streams.

        Workflow:
        1. Generate 3 different protocol captures
        2. Decode all concurrently
        3. Verify all complete successfully
        4. Verify no resource contention
        """
        # Generate 3 different captures
        captures = {}

        for i, protocol in enumerate(["uart", "square", "i2c"]):
            try:
                from oscura.validation.testing.synthetic import SyntheticSignalConfig

                config = SyntheticSignalConfig(
                    pattern_type=cast(
                        "Literal['square', 'uart', 'spi', 'i2c', 'random']", protocol
                    ),
                    sample_rate=10e6,
                    duration_samples=50000,
                )

                signal, _ = generate_digital_signal(pattern=protocol, **config.__dict__)

                file_path = tmp_path / f"{protocol}_capture.bin"
                # Save as binary (convert float to uint8)
                binary_signal = (signal * 255).astype(np.uint8)
                file_path.write_bytes(bytes(binary_signal))

                captures[protocol] = file_path

            except (ImportError, AttributeError):
                pass

        if len(captures) < 2:
            pytest.skip("Signal generation not available")

        # Process all (in real implementation, would use threading/multiprocessing)
        results = {}
        for protocol, file_path in captures.items():
            # Load and analyze
            binary_data = file_path.read_bytes()
            signal = np.frombuffer(binary_data, dtype=np.uint8).astype(np.float64) / 255.0

            metadata = TraceMetadata(sample_rate=10e6)
            trace = DigitalTrace(data=signal > 0.5, metadata=metadata)

            # Detect edges
            detector = EdgeDetector()
            rising, falling = detector.detect_all_edges(trace.data.astype(np.float64))

            results[protocol] = {
                "rising_edges": len(rising),
                "falling_edges": len(falling),
            }

        # Verify all processed
        assert len(results) == len(captures)

        # Verify all found edges
        for protocol, result in results.items():
            total_edges = result["rising_edges"] + result["falling_edges"]
            assert total_edges > 0
