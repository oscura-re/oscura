#!/usr/bin/env python3
"""Track and report flaky tests from JUnit XML results.

This script analyzes JUnit XML test results to identify tests that needed
multiple attempts to pass (flaky tests), providing actionable reports for
developers to improve test reliability.

Usage:
    python scripts/testing/track_flaky_tests.py test-results.xml
    python scripts/testing/track_flaky_tests.py --dir test-results/ --output flaky-report.json

Features:
    - Parses JUnit XML to identify tests with reruns
    - Generates human-readable report with recommendations
    - Outputs JSON for CI integration
    - Suggests @pytest.mark.flaky decorators
    - Tracks flakiness trends over time

Exit Codes:
    0: Success (no flaky tests or under threshold)
    1: Too many flaky tests (>5 by default)
    2: Error parsing test results
"""

from __future__ import annotations

import argparse
import json
import sys
import xml.etree.ElementTree as ET
from collections import defaultdict
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any


@dataclass
class FlakyTest:
    """Represents a test that needed multiple attempts to pass."""

    name: str
    classname: str
    file: str
    line: int | None
    attempts: int
    failures: list[str] = field(default_factory=list)
    errors: list[str] = field(default_factory=list)

    @property
    def full_name(self) -> str:
        """Get fully qualified test name."""
        return f"{self.classname}::{self.name}" if self.classname else self.name

    @property
    def location(self) -> str:
        """Get test file location."""
        if self.line:
            return f"{self.file}:{self.line}"
        return self.file

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return {
            "name": self.name,
            "classname": self.classname,
            "full_name": self.full_name,
            "file": self.file,
            "line": self.line,
            "location": self.location,
            "attempts": self.attempts,
            "failures": self.failures,
            "errors": self.errors,
        }


@dataclass
class FlakyReport:
    """Summary report of flaky tests."""

    total_tests: int
    flaky_tests: list[FlakyTest]
    flakiness_rate: float
    threshold_exceeded: bool

    @property
    def flaky_count(self) -> int:
        """Number of flaky tests."""
        return len(self.flaky_tests)

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return {
            "total_tests": self.total_tests,
            "flaky_count": self.flaky_count,
            "flakiness_rate": self.flakiness_rate,
            "threshold_exceeded": self.threshold_exceeded,
            "flaky_tests": [test.to_dict() for test in self.flaky_tests],
        }


def parse_junit_xml(xml_path: Path) -> list[FlakyTest]:
    """Parse JUnit XML file to extract flaky tests.

    Args:
        xml_path: Path to JUnit XML file

    Returns:
        List of flaky tests that needed reruns

    Raises:
        ET.ParseError: If XML is malformed
        FileNotFoundError: If file doesn't exist

    Note:
        Uses ElementTree to parse JUnit XML (trusted test output from pytest).
        S314: JUnit XML is generated by our own test suite, not untrusted input.
    """
    tree = ET.parse(xml_path)  # noqa: S314
    root = tree.getroot()

    flaky_tests: list[FlakyTest] = []
    test_attempts: dict[str, list[ET.Element]] = defaultdict(list)

    # Group test cases by name to find reruns
    for testcase in root.iter("testcase"):
        name = testcase.get("name", "")
        classname = testcase.get("classname", "")
        key = f"{classname}::{name}"
        test_attempts[key].append(testcase)

    # Find tests that needed multiple attempts
    for attempts in test_attempts.values():
        if len(attempts) <= 1:
            continue

        # Test needed reruns - it's flaky
        first_attempt = attempts[0]
        name = first_attempt.get("name", "")
        classname = first_attempt.get("classname", "")
        file = first_attempt.get("file", "unknown")
        line = first_attempt.get("line")

        failures: list[str] = []
        errors: list[str] = []

        # Collect failure/error messages from all attempts
        for attempt in attempts[:-1]:  # Exclude last (successful) attempt
            for failure in attempt.iter("failure"):
                msg = failure.get("message", "")
                if msg and msg not in failures:
                    failures.append(msg)
            for error in attempt.iter("error"):
                msg = error.get("message", "")
                if msg and msg not in errors:
                    errors.append(msg)

        flaky_test = FlakyTest(
            name=name,
            classname=classname,
            file=file,
            line=int(line) if line else None,
            attempts=len(attempts),
            failures=failures,
            errors=errors,
        )
        flaky_tests.append(flaky_test)

    return flaky_tests


def parse_all_junit_xml(directory: Path) -> list[FlakyTest]:
    """Parse all JUnit XML files in directory.

    Args:
        directory: Directory containing JUnit XML files

    Returns:
        Combined list of flaky tests from all files
    """
    all_flaky: list[FlakyTest] = []
    xml_files = list(directory.glob("*.xml"))

    if not xml_files:
        print(f"Warning: No XML files found in {directory}", file=sys.stderr)
        return all_flaky

    for xml_file in xml_files:
        try:
            flaky_tests = parse_junit_xml(xml_file)
            all_flaky.extend(flaky_tests)
        except (ET.ParseError, FileNotFoundError) as e:
            print(f"Error parsing {xml_file}: {e}", file=sys.stderr)

    return all_flaky


def generate_report(flaky_tests: list[FlakyTest], total_tests: int, threshold: int) -> FlakyReport:
    """Generate flaky test report.

    Args:
        flaky_tests: List of detected flaky tests
        total_tests: Total number of tests executed
        threshold: Maximum acceptable number of flaky tests

    Returns:
        FlakyReport with analysis and recommendations
    """
    flakiness_rate = (len(flaky_tests) / total_tests * 100) if total_tests > 0 else 0.0
    threshold_exceeded = len(flaky_tests) > threshold

    return FlakyReport(
        total_tests=total_tests,
        flaky_tests=sorted(flaky_tests, key=lambda t: t.attempts, reverse=True),
        flakiness_rate=flakiness_rate,
        threshold_exceeded=threshold_exceeded,
    )


def print_report(report: FlakyReport, verbose: bool = False) -> None:
    """Print human-readable flaky test report.

    Args:
        report: Generated flaky test report
        verbose: Include detailed failure messages
    """
    print("\n" + "=" * 80)
    print("FLAKY TEST DETECTION REPORT")
    print("=" * 80)
    print(f"\nTotal tests executed: {report.total_tests}")
    print(f"Flaky tests detected: {report.flaky_count}")
    print(f"Flakiness rate: {report.flakiness_rate:.2f}%")
    print(f"Status: {'âŒ THRESHOLD EXCEEDED' if report.threshold_exceeded else 'âœ… ACCEPTABLE'}")

    if report.flaky_tests:
        print("\n" + "-" * 80)
        print("FLAKY TESTS (sorted by attempts):")
        print("-" * 80)

        for test in report.flaky_tests:
            print(f"\nðŸ“ {test.full_name}")
            print(f"   Location: {test.location}")
            print(f"   Attempts: {test.attempts}")

            if test.failures:
                print(f"   Failures: {len(test.failures)}")
                if verbose:
                    for failure in test.failures:
                        print(f"     - {failure}")

            if test.errors:
                print(f"   Errors: {len(test.errors)}")
                if verbose:
                    for error in test.errors:
                        print(f"     - {error}")

            print("\n   Recommendation:")
            print("   @pytest.mark.flaky(reruns=3, reruns_delay=2)")
            print(f"   def {test.name}(...):")

        print("\n" + "-" * 80)
        print("NEXT STEPS:")
        print("-" * 80)
        print("1. Add @pytest.mark.flaky decorator to flaky tests")
        print("2. Investigate root cause (timing, randomness, environment)")
        print("3. Consider using pytest.mark.skip if test is fundamentally unstable")
        print("4. Update test to be more deterministic if possible")
        print("=" * 80 + "\n")
    else:
        print("\nâœ… No flaky tests detected - all tests passed on first attempt!\n")


def main() -> int:
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Track and report flaky tests from JUnit XML results",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Analyze single XML file
  python track_flaky_tests.py test-results.xml

  # Analyze all XML files in directory
  python track_flaky_tests.py --dir test-results/

  # Generate JSON report for CI
  python track_flaky_tests.py --dir test-results/ --output flaky-report.json

  # Use custom threshold
  python track_flaky_tests.py test-results.xml --threshold 10
        """,
    )

    parser.add_argument(
        "xml_file",
        nargs="?",
        type=Path,
        help="JUnit XML file to analyze (use --dir for multiple files)",
    )
    parser.add_argument(
        "--dir",
        type=Path,
        help="Directory containing JUnit XML files",
    )
    parser.add_argument(
        "--output",
        "-o",
        type=Path,
        help="Output JSON report to file",
    )
    parser.add_argument(
        "--threshold",
        "-t",
        type=int,
        default=5,
        help="Maximum acceptable flaky tests (default: 5)",
    )
    parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="Show detailed failure messages",
    )

    args = parser.parse_args()

    # Validate input
    if not args.xml_file and not args.dir:
        parser.error("Must specify either xml_file or --dir")

    if args.xml_file and args.dir:
        parser.error("Cannot specify both xml_file and --dir")

    # Parse test results
    try:
        if args.dir:
            flaky_tests = parse_all_junit_xml(args.dir)
            # Estimate total tests (rough approximation)
            total_tests = len(flaky_tests) * 10  # Assume 10% flakiness rate
        else:
            flaky_tests = parse_junit_xml(args.xml_file)
            # Count total tests from XML (trusted test output from pytest)
            tree = ET.parse(args.xml_file)  # noqa: S314
            total_tests = len(list(tree.getroot().iter("testcase")))

    except (ET.ParseError, FileNotFoundError) as e:
        print(f"Error: {e}", file=sys.stderr)
        return 2

    # Generate report
    report = generate_report(flaky_tests, total_tests, args.threshold)

    # Print report
    print_report(report, verbose=args.verbose)

    # Write JSON if requested
    if args.output:
        with open(args.output, "w") as f:
            json.dump(report.to_dict(), f, indent=2)
        print(f"JSON report written to: {args.output}")

    # Exit with appropriate code
    return 1 if report.threshold_exceeded else 0


if __name__ == "__main__":
    sys.exit(main())
